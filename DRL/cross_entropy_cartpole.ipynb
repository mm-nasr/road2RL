{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross_entropy_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7taSffwbB_oy"
      },
      "source": [
        "# Cross Entropy for CartPole Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgjhnwpEBOUL",
        "outputId": "ea8ca4ca-5f80-4645-a542-1625926f2976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# !pip install tensorboardX\n",
        "# Import required packages\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 16.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcBfntTrB9tV"
      },
      "source": [
        "## Agent: NN Model\n",
        "\n",
        "One hidden layer NN with 128 neurons and a ReLU activation function.\n",
        "\n",
        "We will use a batch size of 16.\n",
        "\n",
        "For cross-entropy, we will use a 70th percentile as the reward boundary. Keep only the top 30%.\n",
        "\n",
        "The output from the NN is a probability distribution over actions, so a straightforward way to proceed would be to include softmax nonlinearity after the last layer but we don't use it to increase the numerical stability of training.\n",
        "\n",
        "\n",
        "PyTorch class nn.CrossEntropyLoss, combines both softmax and cross-entropy in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the NN (also called logits). The downside of this is that we need to remember to apply softmax every time we need to get probabilities from our NN's output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6llrzcspFiJJ"
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2KmDLBLMf4Q"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, obs_size, hidden_size, n_actions):\n",
        "    super(Net, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCKXVTI6GF4I"
      },
      "source": [
        "## Cross-entropy Algorithm\n",
        "\n",
        "  1. Play N number of episodes using our current model and environment.\n",
        "  2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
        "  3. Throw away all episodes with a reward below the boundary.\n",
        "  4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
        "  5. Repeat from step 1 until we become satisfied with the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "481O-tegGRXj"
      },
      "source": [
        "#  This is a single episode stored as total undiscounted reward and a collection of EpisodeStep\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "#  represent one single step that our agent made in the episode\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
        "\n",
        "def iterate_batches(env, net, batch_size):\n",
        "  '''\n",
        "  Generate batches with episodes\n",
        "  '''\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs = env.reset()\n",
        "  sm = nn.Softmax(dim=1) # to output to probability dist of actions\n",
        "  while True:\n",
        "      obs_v = torch.FloatTensor([obs])\n",
        "      act_probs_v = sm(net(obs_v))\n",
        "      # Both our NN and the softmax layer return tensors that track gradients, \n",
        "      # so we need to unpack this by accessing the tensor.data field \n",
        "      # and then converting the tensor into a NumPy array. \n",
        "      act_probs = act_probs_v.data.numpy()[0]\n",
        "      # Random choice according to obtained probabilities\n",
        "      action = np.random.choice(len(act_probs), p=act_probs) \n",
        "      next_obs, reward, is_done, _ = env.step(action)\n",
        "      episode_reward += reward\n",
        "      step = EpisodeStep(observation=obs, action=action)\n",
        "      episode_steps.append(step)\n",
        "      if is_done:\n",
        "          e = Episode(reward=episode_reward, steps=episode_steps)\n",
        "          batch.append(e)\n",
        "          episode_reward = 0.0\n",
        "          episode_steps = []\n",
        "          next_obs = env.reset()\n",
        "          if len(batch) == batch_size:\n",
        "              yield batch\n",
        "              batch = []\n",
        "      obs = next_obs\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbJOq7jv-yZP"
      },
      "source": [
        "The training of our NN and the generation of our episodes are performed at the same time.\n",
        "\n",
        "Every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the NN using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWbynS7-5do"
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "  rewards = list(map(lambda s: s.reward, batch))\n",
        "  reward_bound = np.percentile(rewards, percentile)\n",
        "  reward_mean = float(np.mean(rewards))\n",
        "\n",
        "  train_obs = []\n",
        "  train_act = []\n",
        "  for reward, steps in batch:\n",
        "      # check that the episode has a higher total reward than our boundary\n",
        "      if reward < reward_bound:\n",
        "          continue\n",
        "      train_obs.extend(map(lambda step: step.observation, steps))\n",
        "      train_act.extend(map(lambda step: step.action, steps))\n",
        "\n",
        "  train_obs_v = torch.FloatTensor(train_obs)\n",
        "  train_act_v = torch.LongTensor(train_act)\n",
        "  return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acaoBVCFQmf",
        "outputId": "9bcdbcc5-72bd-4843-bd36-8c446c565cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  env = gym.make(\"CartPole-v0\")\n",
        "  # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "  obs_size = env.observation_space.shape[0]\n",
        "  n_actions = env.action_space.n\n",
        "\n",
        "  net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "  writer = SummaryWriter(comment=\"-cartpole\")\n",
        "\n",
        "  for iter_no, batch in enumerate(iterate_batches(\n",
        "          env, net, BATCH_SIZE)):\n",
        "      obs_v, acts_v, reward_b, reward_m = \\\n",
        "          filter_batch(batch, PERCENTILE)\n",
        "      optimizer.zero_grad()\n",
        "      action_scores_v = net(obs_v)\n",
        "      loss_v = objective(action_scores_v, acts_v)\n",
        "      loss_v.backward()\n",
        "      optimizer.step()\n",
        "      print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
        "          iter_no, loss_v.item(), reward_m, reward_b))\n",
        "      writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "      writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "      writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "      if reward_m > 199:\n",
        "          print(\"Solved!\")\n",
        "          break\n",
        "  writer.close()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: loss=0.681, reward_mean=16.8, rw_bound=19.0\n",
            "1: loss=0.695, reward_mean=27.9, rw_bound=30.5\n",
            "2: loss=0.674, reward_mean=31.8, rw_bound=42.0\n",
            "3: loss=0.659, reward_mean=30.9, rw_bound=32.0\n",
            "4: loss=0.684, reward_mean=31.3, rw_bound=36.0\n",
            "5: loss=0.648, reward_mean=29.6, rw_bound=27.5\n",
            "6: loss=0.641, reward_mean=33.5, rw_bound=34.5\n",
            "7: loss=0.639, reward_mean=32.7, rw_bound=40.5\n",
            "8: loss=0.636, reward_mean=39.0, rw_bound=48.0\n",
            "9: loss=0.624, reward_mean=43.6, rw_bound=51.0\n",
            "10: loss=0.619, reward_mean=57.2, rw_bound=73.5\n",
            "11: loss=0.603, reward_mean=48.1, rw_bound=58.5\n",
            "12: loss=0.597, reward_mean=58.6, rw_bound=60.0\n",
            "13: loss=0.593, reward_mean=51.1, rw_bound=51.5\n",
            "14: loss=0.586, reward_mean=51.7, rw_bound=61.5\n",
            "15: loss=0.581, reward_mean=52.4, rw_bound=61.0\n",
            "16: loss=0.587, reward_mean=51.7, rw_bound=58.0\n",
            "17: loss=0.583, reward_mean=49.6, rw_bound=55.5\n",
            "18: loss=0.552, reward_mean=51.5, rw_bound=61.0\n",
            "19: loss=0.551, reward_mean=56.6, rw_bound=67.0\n",
            "20: loss=0.546, reward_mean=56.0, rw_bound=71.0\n",
            "21: loss=0.566, reward_mean=49.4, rw_bound=53.5\n",
            "22: loss=0.531, reward_mean=57.3, rw_bound=65.0\n",
            "23: loss=0.576, reward_mean=62.2, rw_bound=68.0\n",
            "24: loss=0.558, reward_mean=62.9, rw_bound=68.5\n",
            "25: loss=0.522, reward_mean=64.9, rw_bound=77.0\n",
            "26: loss=0.536, reward_mean=71.2, rw_bound=81.0\n",
            "27: loss=0.520, reward_mean=59.1, rw_bound=66.5\n",
            "28: loss=0.523, reward_mean=57.6, rw_bound=63.0\n",
            "29: loss=0.513, reward_mean=79.4, rw_bound=104.0\n",
            "30: loss=0.513, reward_mean=78.8, rw_bound=79.0\n",
            "31: loss=0.527, reward_mean=65.1, rw_bound=67.5\n",
            "32: loss=0.499, reward_mean=73.8, rw_bound=87.0\n",
            "33: loss=0.510, reward_mean=65.1, rw_bound=71.0\n",
            "34: loss=0.509, reward_mean=90.6, rw_bound=83.0\n",
            "35: loss=0.522, reward_mean=85.6, rw_bound=92.0\n",
            "36: loss=0.505, reward_mean=81.6, rw_bound=84.5\n",
            "37: loss=0.476, reward_mean=84.6, rw_bound=99.0\n",
            "38: loss=0.490, reward_mean=96.1, rw_bound=105.5\n",
            "39: loss=0.501, reward_mean=85.4, rw_bound=89.5\n",
            "40: loss=0.522, reward_mean=98.1, rw_bound=111.0\n",
            "41: loss=0.480, reward_mean=103.3, rw_bound=107.5\n",
            "42: loss=0.510, reward_mean=93.4, rw_bound=100.5\n",
            "43: loss=0.473, reward_mean=105.0, rw_bound=113.0\n",
            "44: loss=0.486, reward_mean=108.2, rw_bound=117.0\n",
            "45: loss=0.489, reward_mean=105.9, rw_bound=123.5\n",
            "46: loss=0.486, reward_mean=116.9, rw_bound=124.5\n",
            "47: loss=0.474, reward_mean=99.1, rw_bound=114.0\n",
            "48: loss=0.475, reward_mean=119.6, rw_bound=138.0\n",
            "49: loss=0.485, reward_mean=128.9, rw_bound=149.5\n",
            "50: loss=0.506, reward_mean=113.9, rw_bound=120.5\n",
            "51: loss=0.485, reward_mean=116.7, rw_bound=138.5\n",
            "52: loss=0.479, reward_mean=143.7, rw_bound=175.5\n",
            "53: loss=0.494, reward_mean=134.8, rw_bound=164.0\n",
            "54: loss=0.484, reward_mean=150.8, rw_bound=187.0\n",
            "55: loss=0.480, reward_mean=178.3, rw_bound=200.0\n",
            "56: loss=0.480, reward_mean=190.4, rw_bound=200.0\n",
            "57: loss=0.482, reward_mean=190.9, rw_bound=200.0\n",
            "58: loss=0.507, reward_mean=190.6, rw_bound=200.0\n",
            "59: loss=0.483, reward_mean=196.6, rw_bound=200.0\n",
            "60: loss=0.491, reward_mean=200.0, rw_bound=200.0\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqv88cWGmX4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}