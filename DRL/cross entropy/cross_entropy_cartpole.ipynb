{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cross_entropy_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python361064bitetrivenvcondae455d438fc8d450ba44d439e8f7e12af",
      "display_name": "Python 3.6.10 64-bit ('etri_venv': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7taSffwbB_oy"
      },
      "source": [
        "# Cross Entropy for CartPole Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgjhnwpEBOUL",
        "outputId": "8eb3786b-d5b3-4f53-fab4-7bbd7a2080d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "# !pip install tensorboardX\n",
        "# Import required packages\n",
        "import gym\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcBfntTrB9tV"
      },
      "source": [
        "## Agent: NN Model\n",
        "\n",
        "One hidden layer NN with 128 neurons and a ReLU activation function.\n",
        "\n",
        "We will use a batch size of 16.\n",
        "\n",
        "For cross-entropy, we will use a 70th percentile as the reward boundary. Keep only the top 30%.\n",
        "\n",
        "The output from the NN is a probability distribution over actions, so a straightforward way to proceed would be to include softmax nonlinearity after the last layer but we don't use it to increase the numerical stability of training.\n",
        "\n",
        "\n",
        "PyTorch class nn.CrossEntropyLoss, combines both softmax and cross-entropy in a single, more numerically stable expression. CrossEntropyLoss requires raw, unnormalized values from the NN (also called logits). The downside of this is that we need to remember to apply softmax every time we need to get probabilities from our NN's output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6llrzcspFiJJ"
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 16\n",
        "PERCENTILE = 70\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2KmDLBLMf4Q"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, obs_size, hidden_size, n_actions):\n",
        "    super(Net, self).__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, n_actions)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCKXVTI6GF4I"
      },
      "source": [
        "## Cross-entropy Algorithm\n",
        "\n",
        "  1. Play N number of episodes using our current model and environment.\n",
        "  2. Calculate the total reward for every episode and decide on a reward boundary. Usually, we use some percentile of all rewards, such as 50th or 70th.\n",
        "  3. Throw away all episodes with a reward below the boundary.\n",
        "  4. Train on the remaining \"elite\" episodes using observations as the input and issued actions as the desired output.\n",
        "  5. Repeat from step 1 until we become satisfied with the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "481O-tegGRXj"
      },
      "source": [
        "#  This is a single episode stored as total undiscounted reward and a collection of EpisodeStep\n",
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "#  represent one single step that our agent made in the episode\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
        "\n",
        "def iterate_batches(env, net, batch_size):\n",
        "  '''\n",
        "  Generate batches with episodes\n",
        "  '''\n",
        "  batch = []\n",
        "  episode_reward = 0.0\n",
        "  episode_steps = []\n",
        "  obs = env.reset()\n",
        "  sm = nn.Softmax(dim=1) # to output to probability dist of actions\n",
        "  while True:\n",
        "      obs_v = torch.FloatTensor([obs])\n",
        "      act_probs_v = sm(net(obs_v))\n",
        "      # Both our NN and the softmax layer return tensors that track gradients, \n",
        "      # so we need to unpack this by accessing the tensor.data field \n",
        "      # and then converting the tensor into a NumPy array. \n",
        "      act_probs = act_probs_v.data.numpy()[0]\n",
        "      # Random choice according to obtained probabilities\n",
        "      action = np.random.choice(len(act_probs), p=act_probs) \n",
        "      next_obs, reward, is_done, _ = env.step(action)\n",
        "      episode_reward += reward\n",
        "      step = EpisodeStep(observation=obs, action=action)\n",
        "      episode_steps.append(step)\n",
        "      if is_done:\n",
        "          e = Episode(reward=episode_reward, steps=episode_steps)\n",
        "          batch.append(e)\n",
        "          episode_reward = 0.0\n",
        "          episode_steps = []\n",
        "          next_obs = env.reset()\n",
        "          if len(batch) == batch_size:\n",
        "              yield batch\n",
        "              batch = []\n",
        "      obs = next_obs\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbJOq7jv-yZP"
      },
      "source": [
        "The training of our NN and the generation of our episodes are performed at the same time.\n",
        "\n",
        "Every time our loop accumulates enough episodes (16), it passes control to this function caller, which is supposed to train the NN using gradient descent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIWbynS7-5do"
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "  rewards = list(map(lambda s: s.reward, batch))\n",
        "  reward_bound = np.percentile(rewards, percentile)\n",
        "  reward_mean = float(np.mean(rewards))\n",
        "\n",
        "  train_obs = []\n",
        "  train_act = []\n",
        "  for reward, steps in batch:\n",
        "      # check that the episode has a higher total reward than our boundary\n",
        "      if reward < reward_bound:\n",
        "          continue\n",
        "      # add the observationa and action to the training lists\n",
        "      train_obs.extend(map(lambda step: step.observation, steps))\n",
        "      train_act.extend(map(lambda step: step.action, steps))\n",
        "\n",
        "  # convert the lists to float tensor for use with the NN in PyTorch\n",
        "  train_obs_v = torch.FloatTensor(train_obs)\n",
        "  train_act_v = torch.LongTensor(train_act)\n",
        "  return train_obs_v, train_act_v, reward_bound, reward_mean"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "source": [
        "## Main training loop\n",
        "\n",
        "1. Initialize environment and NN agent (as well as optimizer and loss function)\n",
        "2. Start a loop by creating batches using *iterate_batches*. The loop stops only when the target mean reward is reached.\n",
        "3. Clean the batch to make sure only those above reward bound are used.\n",
        "4. Train the NN on the batch for 1 epoch as follows (Standard backprop process):\n",
        "\n",
        "    a) Forward pass\n",
        "\n",
        "    b) Calculate loss\n",
        "    \n",
        "    c) Backpropagate loss and update weights"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1acaoBVCFQmf",
        "outputId": "58c127db-0449-4ee4-b37e-190a53947f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  target_mean = 199\n",
        "  env = gym.make(\"CartPole-v0\")\n",
        "#   NOTE: downgrade pyglet to 1.3.2 -- Otherwise it breaks gym\n",
        "#   install ffmpeg as well\n",
        "#   env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
        "  obs_size = env.observation_space.shape[0]\n",
        "  n_actions = env.action_space.n\n",
        "\n",
        "  net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "  objective = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
        "  writer = SummaryWriter(comment=\"-cartpole\")\n",
        "\n",
        "  for iter_no, batch in enumerate(iterate_batches(\n",
        "          env, net, BATCH_SIZE)):\n",
        "      # filter the batch to get only those above the reward boundary\n",
        "      obs_v, acts_v, reward_b, reward_m = \\\n",
        "          filter_batch(batch, PERCENTILE)\n",
        "      # train on the filtered batch for 1 epoch\n",
        "      optimizer.zero_grad()\n",
        "      action_scores_v = net(obs_v)\n",
        "      loss_v = objective(action_scores_v, acts_v)\n",
        "      loss_v.backward()\n",
        "      optimizer.step()\n",
        "      print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
        "          iter_no, loss_v.item(), reward_m, reward_b))\n",
        "      writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
        "      writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
        "      writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
        "      if reward_m > target_mean:\n",
        "          print(\"Solved!\")\n",
        "          break\n",
        "  writer.close()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "0: loss=0.690, reward_mean=17.6, rw_bound=20.5\n",
            "1: loss=0.672, reward_mean=22.2, rw_bound=23.0\n",
            "2: loss=0.662, reward_mean=28.3, rw_bound=28.0\n",
            "3: loss=0.649, reward_mean=25.8, rw_bound=30.0\n",
            "4: loss=0.642, reward_mean=43.3, rw_bound=37.0\n",
            "5: loss=0.628, reward_mean=52.4, rw_bound=67.5\n",
            "6: loss=0.618, reward_mean=64.2, rw_bound=69.5\n",
            "7: loss=0.625, reward_mean=47.8, rw_bound=58.5\n",
            "8: loss=0.608, reward_mean=63.0, rw_bound=65.0\n",
            "9: loss=0.602, reward_mean=63.2, rw_bound=73.5\n",
            "10: loss=0.582, reward_mean=62.8, rw_bound=73.5\n",
            "11: loss=0.584, reward_mean=79.3, rw_bound=74.0\n",
            "12: loss=0.575, reward_mean=69.8, rw_bound=84.0\n",
            "13: loss=0.585, reward_mean=85.1, rw_bound=96.5\n",
            "14: loss=0.582, reward_mean=90.9, rw_bound=103.0\n",
            "15: loss=0.579, reward_mean=113.4, rw_bound=139.5\n",
            "16: loss=0.563, reward_mean=95.8, rw_bound=101.0\n",
            "17: loss=0.565, reward_mean=112.3, rw_bound=158.0\n",
            "18: loss=0.550, reward_mean=109.1, rw_bound=126.0\n",
            "19: loss=0.556, reward_mean=148.6, rw_bound=173.5\n",
            "20: loss=0.565, reward_mean=177.8, rw_bound=200.0\n",
            "21: loss=0.547, reward_mean=178.2, rw_bound=200.0\n",
            "22: loss=0.563, reward_mean=174.4, rw_bound=200.0\n",
            "23: loss=0.553, reward_mean=171.3, rw_bound=200.0\n",
            "24: loss=0.536, reward_mean=144.9, rw_bound=175.5\n",
            "25: loss=0.545, reward_mean=161.9, rw_bound=188.5\n",
            "26: loss=0.543, reward_mean=181.2, rw_bound=200.0\n",
            "27: loss=0.541, reward_mean=182.4, rw_bound=200.0\n",
            "28: loss=0.542, reward_mean=174.5, rw_bound=200.0\n",
            "29: loss=0.532, reward_mean=191.6, rw_bound=200.0\n",
            "30: loss=0.534, reward_mean=193.5, rw_bound=200.0\n",
            "31: loss=0.535, reward_mean=195.2, rw_bound=200.0\n",
            "32: loss=0.524, reward_mean=192.2, rw_bound=200.0\n",
            "33: loss=0.524, reward_mean=184.1, rw_bound=200.0\n",
            "34: loss=0.519, reward_mean=196.1, rw_bound=200.0\n",
            "35: loss=0.518, reward_mean=200.0, rw_bound=200.0\n",
            "Solved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEqv88cWGmX4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}